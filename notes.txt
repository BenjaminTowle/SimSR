Dialogue retrieval

situation: current retrieval methods involve two-stage paradigm with one-to-many task of retrieving set of candidates
followed by a fine-grained reranker

complication: current methods train only on a one-to-one task of predicting mostly likely response, rather than
predicting best set of responses. This is further complicated by the fact that no ground truth set of responses exists to train on,
nor is it immediately obvious what architecture should be used.

solution: present a method that uses model-based simulation to learn good sets of candidates, with novel clustering algorithm for
searching for candidates, and novel architecture for learning to predict sets of candidates.



Smart Reply

situation: smart reply involves selecting a small set of k candidates to the user in response to a message, of which
at least one must be useful. Different from dialogue in which the model must commit to a single response, the model
is affored multiple predictions owing to the nature of the task.

complication: despite the task being a one-to-many task, most datasets only provide a single target response per context to train on.
This is a problem because the usefulness of each candidate is dependent on the other candidates in the selection: e.g. "that's great",
provides limited incremental value if "that's good" is already in the selection. As a result, current methods rely on either heuristics to encourage diversity in predictions, 
or difficult to optimise latent variable methods; however, while these methods do indeed improve diversity, they do not optimise the task
of selecting the optimal set of candidates. 

solution: present a method that uses model-based simulation to learn good sets of candidates, with novel clustering algorithm for
searching for candidates, and novel architecture for learning to predict sets of candidates.


The task of smart reply differs crucially from dialogue in that only one of the suggestions needs to be useful. As a result, selecting only the top-k individual
candidates will only achieve a lower bound of performance. For instance, suggesting "that's great" provides limited incremental value when "that's good" is already a candidate.
This is conceptually the same as the idea of maximum marginal relevance in Information Retrieval, which scores documents by also considering their incremental utility given
the documents already retrieved.

